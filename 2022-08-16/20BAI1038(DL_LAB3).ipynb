{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e93d13",
   "metadata": {},
   "source": [
    "# CNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6a9e1",
   "metadata": {},
   "source": [
    "#### Name: Vishal L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e282b5",
   "metadata": {},
   "source": [
    "#### Reg.No: 20BAI1038"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d04b2e",
   "metadata": {},
   "source": [
    "### MNIST DATASET: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39461c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583e2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate accuracy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b5aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e7dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the input vector from the 28x28 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba40ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969453c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (60000,)\n",
      "Shape after one-hot encoding:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483eb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3eb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b0b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output of conv\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5aa67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe90f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8aea127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc8e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 25s 50ms/step - loss: 0.2010 - accuracy: 0.9426 - val_loss: 0.0786 - val_accuracy: 0.9763\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 23s 49ms/step - loss: 0.0622 - accuracy: 0.9811 - val_loss: 0.0608 - val_accuracy: 0.9798\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 23s 48ms/step - loss: 0.0391 - accuracy: 0.9880 - val_loss: 0.0537 - val_accuracy: 0.9816\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 23s 49ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0485 - val_accuracy: 0.9835\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.0482 - val_accuracy: 0.9853\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 23s 49ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0525 - val_accuracy: 0.9849\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 23s 48ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.0571 - val_accuracy: 0.9839\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 21s 45ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0575 - val_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0614 - val_accuracy: 0.9827\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0700 - val_accuracy: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17d1886b910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc829e0",
   "metadata": {},
   "source": [
    "#### Even though our max validation accuracy by using a simple neural network model was around 97%, the CNN model is able to get 98%+ with just a single convolution layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d87cdd",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2b636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a7c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 142s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9a62cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # building the input vector from the 32x32 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "982b64a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (50000, 1)\n",
      "Shape after one-hot encoding:  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b9d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef8d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85da5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "045367d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a637b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output of conv\n",
    "model.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "134bc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b5e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b422dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8621dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 1.5843 - accuracy: 0.4176 - val_loss: 1.2420 - val_accuracy: 0.5613\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 176s 450ms/step - loss: 1.1313 - accuracy: 0.5980 - val_loss: 1.0447 - val_accuracy: 0.6382\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 181s 463ms/step - loss: 0.9331 - accuracy: 0.6707 - val_loss: 0.8222 - val_accuracy: 0.7190\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 171s 437ms/step - loss: 0.8154 - accuracy: 0.7165 - val_loss: 0.7589 - val_accuracy: 0.7362\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 171s 438ms/step - loss: 0.7308 - accuracy: 0.7447 - val_loss: 0.7011 - val_accuracy: 0.7560\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 177s 453ms/step - loss: 0.6604 - accuracy: 0.7702 - val_loss: 0.6769 - val_accuracy: 0.7655\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 169s 433ms/step - loss: 0.6079 - accuracy: 0.7871 - val_loss: 0.6601 - val_accuracy: 0.7762\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 172s 440ms/step - loss: 0.5524 - accuracy: 0.8036 - val_loss: 0.6407 - val_accuracy: 0.7822\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 174s 445ms/step - loss: 0.5129 - accuracy: 0.8194 - val_loss: 0.6755 - val_accuracy: 0.7744\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 170s 434ms/step - loss: 0.4807 - accuracy: 0.8301 - val_loss: 0.6742 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17d19b264d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca703fef",
   "metadata": {},
   "source": [
    "## Arabic Handwritten digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ab27250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPool2D,Dropout,LSTM\n",
    "from keras.optimizers import RMSprop, Adam, Nadam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b73b63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('csvTrainLabel 60k x 1.csv')\n",
    "X_train = pd.read_csv('csvTrainImages 60k x 784.csv')\n",
    "y_test = pd.read_csv('csvTestLabel 10k x 1.csv')\n",
    "X_test = pd.read_csv('csvTestImages 10k x 784.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "572325f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise and reshape\n",
    "X_train=X_train.values.reshape((-1,28,28,1))/255.0\n",
    "X_test=X_test.values.reshape((-1,28,28,1))/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8660e0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OnehotEncode y_train\n",
    "y_train=to_categorical(y_train, num_classes=10)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27aba82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and validation\n",
    "X_train,X_valid,y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                     test_size=0.1,\n",
    "                                                     shuffle=True)\n",
    "# Data Augmentation to reduce bias\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        #rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64eedec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class custom_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if(logs.get('accuracy')>0.92):\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "# LR Scheduler\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e62796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same',\n",
    "                activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same',\n",
    "                activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),padding='Same',\n",
    "                activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3),padding='Same',\n",
    "                activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout((0.4)))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55a64fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# Compile the model\n",
    "model = build_model()\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b061d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 128s - loss: 0.1783 - accuracy: 0.9451 - val_loss: 0.0351 - val_accuracy: 0.9898 - lr: 0.0010 - 128s/epoch - 151ms/step\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 129s - loss: 0.0564 - accuracy: 0.9837 - val_loss: 0.0298 - val_accuracy: 0.9937 - lr: 0.0010 - 129s/epoch - 153ms/step\n",
      "Epoch 3/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 129s - loss: 0.0491 - accuracy: 0.9868 - val_loss: 0.0258 - val_accuracy: 0.9943 - lr: 0.0010 - 129s/epoch - 152ms/step\n",
      "Epoch 4/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 128s - loss: 0.0461 - accuracy: 0.9876 - val_loss: 0.0284 - val_accuracy: 0.9953 - lr: 0.0010 - 128s/epoch - 152ms/step\n",
      "Epoch 5/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 129s - loss: 0.0467 - accuracy: 0.9880 - val_loss: 0.0221 - val_accuracy: 0.9952 - lr: 0.0010 - 129s/epoch - 152ms/step\n",
      "Epoch 6/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 128s - loss: 0.0465 - accuracy: 0.9887 - val_loss: 0.0250 - val_accuracy: 0.9943 - lr: 0.0010 - 128s/epoch - 152ms/step\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 132s - loss: 0.0460 - accuracy: 0.9887 - val_loss: 0.0255 - val_accuracy: 0.9945 - lr: 0.0010 - 132s/epoch - 156ms/step\n",
      "Epoch 8/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 163s - loss: 0.0446 - accuracy: 0.9897 - val_loss: 0.0226 - val_accuracy: 0.9940 - lr: 0.0010 - 163s/epoch - 194ms/step\n",
      "Epoch 9/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 151s - loss: 0.0490 - accuracy: 0.9889 - val_loss: 0.0374 - val_accuracy: 0.9935 - lr: 0.0010 - 151s/epoch - 179ms/step\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "844/844 - 153s - loss: 0.0485 - accuracy: 0.9890 - val_loss: 0.0280 - val_accuracy: 0.9942 - lr: 0.0010 - 153s/epoch - 181ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=64\n",
    "# Fit the model\n",
    "history = model.fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_valid,y_valid),\n",
    "                              verbose = 2, callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3258f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0491 - accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.049127764999866486, 0.9916991591453552]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test = to_categorical(y_test,num_classes=10)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d599405",
   "metadata": {},
   "source": [
    "### using Hyperas we increased the accuracy from 0.9451 to 0.9917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef241d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
